{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Neural Network Classification Project\n",
        "\n",
        "================================================================================\n",
        "\n",
        "A comprehensive implementation of feed-forward neural networks for image classification\n",
        "using TensorFlow/Keras on the MNIST dataset.\n",
        "\n",
        "Author: AKAKPO Koffi Moïse\n",
        "\n",
        "Date: August 2025\n",
        "\n",
        "Purpose: Machine Learning Internship Application - Task 3"
      ],
      "metadata": {
        "id": "qTR8HrItm1YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installation de tensorflow\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r7ZguBBLn7jA",
        "outputId": "3afb56b5-6472-492a-ba1e-5051bad0f7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "s1JWXLyGnMh1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "paX1DFDYyikH"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkClassifier:\n",
        "    \"\"\"\n",
        "    A comprehensive neural network classifier with hyperparameter tuning capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize the neural network classifier.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of input data\n",
        "            num_classes (int): Number of output classes\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.best_model = None  # ✅ Attribut pour stocker le meilleur modèle\n",
        "        self.best_params = None  # ✅ Attribut pour stocker les meilleurs paramètres\n",
        "        self.best_accuracy = 0   # ✅ Attribut pour stocker la meilleure accuracy\n",
        "\n",
        "    def create_model(self, hidden_layers=[128, 64], dropout_rate=0.2, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Create a feed-forward neural network architecture.\n",
        "\n",
        "        Args:\n",
        "            hidden_layers (list): List of hidden layer sizes\n",
        "            dropout_rate (float): Dropout rate for regularization\n",
        "            learning_rate (float): Learning rate for optimizer\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Flatten(input_shape=self.input_shape),\n",
        "            layers.Dense(hidden_layers[0], activation='relu', name='hidden_1'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(hidden_layers[1], activation='relu', name='hidden_2'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(self.num_classes, activation='softmax', name='output')\n",
        "        ])\n",
        "\n",
        "        # Compile with appropriate optimizer and metrics\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, verbose=1):\n",
        "        \"\"\"\n",
        "        Train the neural network with early stopping and learning rate scheduling.\n",
        "\n",
        "        Args:\n",
        "            X_train, y_train: Training data\n",
        "            X_val, y_val: Validation data\n",
        "            epochs (int): Maximum number of epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            verbose (int): Verbosity level\n",
        "        \"\"\"\n",
        "        # Callbacks for better training\n",
        "        callbacks = [\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train the model\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data.\n",
        "\n",
        "        Args:\n",
        "            X_test, y_test: Test data\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred_proba = self.model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Classification report\n",
        "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # Confusion matrix\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        return {\n",
        "            'test_loss': test_loss,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_pred_proba,\n",
        "            'classification_report': class_report,\n",
        "            'confusion_matrix': conf_matrix\n",
        "        }\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train, y_train, X_val, y_val, param_grid=None):\n",
        "        \"\"\"\n",
        "        Perform hyperparameter tuning to find optimal parameters and save the best model.\n",
        "\n",
        "        Args:\n",
        "            X_train, y_train: Training data\n",
        "            X_val, y_val: Validation data\n",
        "            param_grid (dict): Dictionary of hyperparameters to test\n",
        "\n",
        "        Returns:\n",
        "            tuple: (best_params, results_df)\n",
        "        \"\"\"\n",
        "        print(\"Starting hyperparameter tuning...\")\n",
        "\n",
        "        # Default parameter grid if none provided\n",
        "        if param_grid is None:\n",
        "            param_grid = {\n",
        "                'learning_rate': [0.001, 0.01, 0.0001],\n",
        "                'batch_size': [32, 64, 128],\n",
        "                'hidden_layers': [[64, 32], [128, 64], [256, 128]]\n",
        "            }\n",
        "\n",
        "        best_accuracy = 0\n",
        "        best_params = {}\n",
        "        best_model = None\n",
        "        results = []\n",
        "\n",
        "        # Grid search\n",
        "        for lr in param_grid['learning_rate']:\n",
        "            for batch_size in param_grid['batch_size']:\n",
        "                for hidden_layers in param_grid['hidden_layers']:\n",
        "                    print(f\"Testing: LR={lr}, Batch={batch_size}, Hidden={hidden_layers}\")\n",
        "\n",
        "                    # Create and train model\n",
        "                    temp_model = self._create_temp_model(hidden_layers=hidden_layers, learning_rate=lr)\n",
        "\n",
        "                    history = temp_model.fit(\n",
        "                        X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=20,\n",
        "                        batch_size=batch_size,\n",
        "                        callbacks=[\n",
        "                            keras.callbacks.EarlyStopping(\n",
        "                                monitor='val_loss',\n",
        "                                patience=5,\n",
        "                                restore_best_weights=True,\n",
        "                                verbose=0\n",
        "                            )\n",
        "                        ],\n",
        "                        verbose=0\n",
        "                    )\n",
        "\n",
        "                    # Get best validation accuracy\n",
        "                    val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "                    results.append({\n",
        "                        'learning_rate': lr,\n",
        "                        'batch_size': batch_size,\n",
        "                        'hidden_layers': str(hidden_layers),\n",
        "                        'val_accuracy': val_accuracy\n",
        "                    })\n",
        "\n",
        "                    # ✅ Update best model if current is better\n",
        "                    if val_accuracy > best_accuracy:\n",
        "                        best_accuracy = val_accuracy\n",
        "                        best_params = {\n",
        "                            'learning_rate': lr,\n",
        "                            'batch_size': batch_size,\n",
        "                            'hidden_layers': hidden_layers\n",
        "                        }\n",
        "                        # Clone the best model\n",
        "                        best_model = tf.keras.models.clone_model(temp_model)\n",
        "                        best_model.set_weights(temp_model.get_weights())\n",
        "                        best_model.compile(\n",
        "                            optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                            loss='sparse_categorical_crossentropy',\n",
        "                            metrics=['accuracy']\n",
        "                        )\n",
        "\n",
        "        # ✅ Store best results in instance attributes\n",
        "        self.best_model = best_model\n",
        "        self.best_params = best_params\n",
        "        self.best_accuracy = best_accuracy\n",
        "\n",
        "        # Convert results to DataFrame for better visualization\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nHyperparameter Tuning Results:\")\n",
        "        print(results_df.sort_values('val_accuracy', ascending=False))\n",
        "\n",
        "        print(f\"\\nBest Parameters: {best_params}\")\n",
        "        print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "        return best_params, results_df\n",
        "\n",
        "    def _create_temp_model(self, hidden_layers=[128, 64], dropout_rate=0.2, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Create a temporary model for hyperparameter tuning.\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Flatten(input_shape=self.input_shape),\n",
        "            layers.Dense(hidden_layers[0], activation='relu', name='hidden_1'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(hidden_layers[1], activation='relu', name='hidden_2'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(self.num_classes, activation='softmax', name='output')\n",
        "        ])\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_best_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the best model to a file using pickle.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the model\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No best model found. Run hyperparameter_tuning first.\")\n",
        "\n",
        "        model_data = {\n",
        "            'model': self.best_model,\n",
        "            'best_params': self.best_params,\n",
        "            'best_accuracy': self.best_accuracy,\n",
        "            'input_shape': self.input_shape,\n",
        "            'num_classes': self.num_classes\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(model_data, f)\n",
        "\n",
        "        print(f\"✅ Best model saved to: {filepath}\")\n",
        "        print(f\"   - Accuracy: {self.best_accuracy:.4f}\")\n",
        "        print(f\"   - Parameters: {self.best_params}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_best_model(cls, filepath):\n",
        "        \"\"\"\n",
        "        Load a saved best model.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the saved model\n",
        "\n",
        "        Returns:\n",
        "            NeuralNetworkClassifier: Loaded classifier instance\n",
        "        \"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            model_data = pickle.load(f)\n",
        "\n",
        "        # Create new instance\n",
        "        classifier = cls(model_data['input_shape'], model_data['num_classes'])\n",
        "        classifier.best_model = model_data['model']\n",
        "        classifier.best_params = model_data['best_params']\n",
        "        classifier.best_accuracy = model_data['best_accuracy']\n",
        "\n",
        "        print(f\"✅ Best model loaded from: {filepath}\")\n",
        "        print(f\"   - Accuracy: {classifier.best_accuracy:.4f}\")\n",
        "        print(f\"   - Parameters: {classifier.best_params}\")\n",
        "\n",
        "        return classifier\n",
        "\n",
        "    def predict_with_best_model(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the best model.\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "\n",
        "        Returns:\n",
        "            dict: Predictions and probabilities\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No best model found. Run hyperparameter_tuning first.\")\n",
        "\n",
        "        probabilities = self.best_model.predict(X, verbose=0)\n",
        "        predictions = np.argmax(probabilities, axis=1)\n",
        "\n",
        "        return {\n",
        "            'predictions': predictions,\n",
        "            'probabilities': probabilities\n",
        "        }\n",
        "\n",
        "    def get_model_summary(self):\n",
        "        \"\"\"\n",
        "        Get a summary of the best model and parameters.\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary information\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            return {\"status\": \"No best model found. Run hyperparameter_tuning first.\"}\n",
        "\n",
        "        return {\n",
        "            'status': 'Best model available',\n",
        "            'best_accuracy': self.best_accuracy,\n",
        "            'best_params': self.best_params,\n",
        "            'input_shape': self.input_shape,\n",
        "            'num_classes': self.num_classes,\n",
        "            'model_layers': [layer.name for layer in self.best_model.layers]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Load and preprocess the MNIST dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Preprocessed training and testing data\n",
        "    \"\"\"\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "\n",
        "    # Load data\n",
        "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Split training data into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"Image shape: {X_train.shape[1:]}\")\n",
        "    print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Exemple d'utilisation avec sauvegarde du meilleur modèle.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"NEURAL NETWORK CLASSIFICATION PROJECT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_and_preprocess_data()\n",
        "\n",
        "    # Create classifier\n",
        "    nn_classifier = NeuralNetworkClassifier(input_shape=(28, 28), num_classes=10)\n",
        "\n",
        "    # Hyperparameter tuning (automatically saves best model)\n",
        "    print(\"\\n2. Hyperparameter Tuning\")\n",
        "    best_params, tuning_results = nn_classifier.hyperparameter_tuning(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # ✅ Save the best model\n",
        "    nn_classifier.save_best_model('best_mnist_model.pkl')\n",
        "\n",
        "    # ✅ Test loading the model\n",
        "    print(\"\\n3. Testing Model Loading\")\n",
        "    loaded_classifier = NeuralNetworkClassifier.load_best_model('best_mnist_model.pkl')\n",
        "\n",
        "    # ✅ Make predictions with best model\n",
        "    print(\"\\n4. Making Predictions with Best Model\")\n",
        "    results = loaded_classifier.predict_with_best_model(X_test[:100])\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = loaded_classifier.best_model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy with Best Model: {test_accuracy:.4f}\")\n",
        "\n",
        "    # ✅ Display model summary\n",
        "    print(\"\\n5. Model Summary\")\n",
        "    summary = loaded_classifier.get_model_summary()\n",
        "    for key, value in summary.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ BEST MODEL SUCCESSFULLY SAVED AND LOADED!\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "5fHHux8t7iZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCQdBvCKmu3M",
        "outputId": "8f96d2ad-3f95-4e9f-dfd3-eaeff629b649"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "NEURAL NETWORK CLASSIFICATION PROJECT\n",
            "============================================================\n",
            "Loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training set: 48000 samples\n",
            "Validation set: 12000 samples\n",
            "Test set: 10000 samples\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 10\n",
            "\n",
            "2. Hyperparameter Tuning\n",
            "Starting hyperparameter tuning...\n",
            "Testing: LR=0.001, Batch=32, Hidden=[64, 32]\n",
            "Testing: LR=0.001, Batch=32, Hidden=[128, 64]\n",
            "Testing: LR=0.001, Batch=32, Hidden=[256, 128]\n",
            "Testing: LR=0.001, Batch=64, Hidden=[64, 32]\n",
            "Testing: LR=0.001, Batch=64, Hidden=[128, 64]\n",
            "Testing: LR=0.001, Batch=64, Hidden=[256, 128]\n",
            "Testing: LR=0.001, Batch=128, Hidden=[64, 32]\n",
            "Testing: LR=0.001, Batch=128, Hidden=[128, 64]\n",
            "Testing: LR=0.001, Batch=128, Hidden=[256, 128]\n",
            "Testing: LR=0.01, Batch=32, Hidden=[64, 32]\n",
            "Testing: LR=0.01, Batch=32, Hidden=[128, 64]\n",
            "Testing: LR=0.01, Batch=32, Hidden=[256, 128]\n",
            "Testing: LR=0.01, Batch=64, Hidden=[64, 32]\n",
            "Testing: LR=0.01, Batch=64, Hidden=[128, 64]\n",
            "Testing: LR=0.01, Batch=64, Hidden=[256, 128]\n",
            "Testing: LR=0.01, Batch=128, Hidden=[64, 32]\n",
            "Testing: LR=0.01, Batch=128, Hidden=[128, 64]\n",
            "Testing: LR=0.01, Batch=128, Hidden=[256, 128]\n",
            "Testing: LR=0.0001, Batch=32, Hidden=[64, 32]\n",
            "Testing: LR=0.0001, Batch=32, Hidden=[128, 64]\n",
            "Testing: LR=0.0001, Batch=32, Hidden=[256, 128]\n",
            "Testing: LR=0.0001, Batch=64, Hidden=[64, 32]\n",
            "Testing: LR=0.0001, Batch=64, Hidden=[128, 64]\n",
            "Testing: LR=0.0001, Batch=64, Hidden=[256, 128]\n",
            "Testing: LR=0.0001, Batch=128, Hidden=[64, 32]\n",
            "Testing: LR=0.0001, Batch=128, Hidden=[128, 64]\n",
            "Testing: LR=0.0001, Batch=128, Hidden=[256, 128]\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "    learning_rate  batch_size hidden_layers  val_accuracy\n",
            "8          0.0010         128    [256, 128]      0.980500\n",
            "5          0.0010          64    [256, 128]      0.979833\n",
            "1          0.0010          32     [128, 64]      0.979333\n",
            "4          0.0010          64     [128, 64]      0.979250\n",
            "20         0.0001          32    [256, 128]      0.979083\n",
            "7          0.0010         128     [128, 64]      0.978333\n",
            "2          0.0010          32    [256, 128]      0.978167\n",
            "23         0.0001          64    [256, 128]      0.977750\n",
            "19         0.0001          32     [128, 64]      0.974250\n",
            "6          0.0010         128      [64, 32]      0.974083\n",
            "3          0.0010          64      [64, 32]      0.973333\n",
            "26         0.0001         128    [256, 128]      0.973167\n",
            "0          0.0010          32      [64, 32]      0.971583\n",
            "17         0.0100         128    [256, 128]      0.970000\n",
            "22         0.0001          64     [128, 64]      0.969583\n",
            "16         0.0100         128     [128, 64]      0.967333\n",
            "13         0.0100          64     [128, 64]      0.965083\n",
            "15         0.0100         128      [64, 32]      0.964750\n",
            "25         0.0001         128     [128, 64]      0.963417\n",
            "14         0.0100          64    [256, 128]      0.963417\n",
            "18         0.0001          32      [64, 32]      0.962583\n",
            "10         0.0100          32     [128, 64]      0.958417\n",
            "12         0.0100          64      [64, 32]      0.958083\n",
            "21         0.0001          64      [64, 32]      0.955583\n",
            "11         0.0100          32    [256, 128]      0.952750\n",
            "9          0.0100          32      [64, 32]      0.949500\n",
            "24         0.0001         128      [64, 32]      0.948083\n",
            "\n",
            "Best Parameters: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "Best Validation Accuracy: 0.9805\n",
            "✅ Best model saved to: best_mnist_model.pkl\n",
            "   - Accuracy: 0.9805\n",
            "   - Parameters: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "\n",
            "3. Testing Model Loading\n",
            "✅ Best model loaded from: best_mnist_model.pkl\n",
            "   - Accuracy: 0.9805\n",
            "   - Parameters: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "\n",
            "4. Making Predictions with Best Model\n",
            "Test Accuracy with Best Model: 0.9787\n",
            "\n",
            "5. Model Summary\n",
            "status: Best model available\n",
            "best_accuracy: 0.9804999828338623\n",
            "best_params: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "input_shape: (28, 28)\n",
            "num_classes: 10\n",
            "model_layers: ['flatten_8', 'hidden_1', 'dropout_16', 'hidden_2', 'dropout_17', 'output']\n",
            "\n",
            "============================================================\n",
            "✅ BEST MODEL SUCCESSFULLY SAVED AND LOADED!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTPredictor:\n",
        "    \"\"\"\n",
        "    Interface professionnelle pour la prédiction de chiffres manuscrits MNIST.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path='best_mnist_model.pkl'):\n",
        "        \"\"\"\n",
        "        Initialise le prédicteur avec le modèle sauvegardé.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Chemin vers le modèle sauvegardé\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        self.model_info = None\n",
        "        self.prediction_history = []\n",
        "\n",
        "        # Charger le modèle\n",
        "        self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Charge le modèle TensorFlow depuis le fichier pickle.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Chemin vers le modèle sauvegardé\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(model_path, 'rb') as f:\n",
        "                model_data = pickle.load(f)\n",
        "\n",
        "            self.model = model_data['model']\n",
        "            self.model_info = {\n",
        "                'best_accuracy': model_data['best_accuracy'],\n",
        "                'best_params': model_data['best_params'],\n",
        "                'input_shape': model_data['input_shape'],\n",
        "                'num_classes': model_data['num_classes']\n",
        "            }\n",
        "\n",
        "            print(f\"✅ Modèle chargé avec succès!\")\n",
        "            print(f\"   - Précision: {self.model_info['best_accuracy']:.4f}\")\n",
        "            print(f\"   - Paramètres: {self.model_info['best_params']}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ Fichier modèle non trouvé: {model_path}\")\n",
        "            print(\"   Assurez-vous d'avoir exécuté l'entraînement d'abord.\")\n",
        "            self.model = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur lors du chargement du modèle: {e}\")\n",
        "            self.model = None\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"\n",
        "        Prétraite l'image pour la prédiction.\n",
        "\n",
        "        Args:\n",
        "            image: Image PIL ou array numpy\n",
        "\n",
        "        Returns:\n",
        "            np.array: Image prétraitée\n",
        "        \"\"\"\n",
        "        # Convertir PIL en numpy si nécessaire\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Convertir en niveaux de gris si couleur\n",
        "        if len(image.shape) == 3:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Redimensionner à 28x28\n",
        "        image = cv2.resize(image, (28, 28))\n",
        "\n",
        "        # Inverser les couleurs (fond noir, écriture blanche)\n",
        "        image = 255 - image\n",
        "\n",
        "        # Normaliser entre 0 et 1\n",
        "        image = image.astype('float32') / 255.0\n",
        "\n",
        "        # Ajouter dimension batch\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def predict(self, image):\n",
        "        \"\"\"\n",
        "        Effectue la prédiction sur l'image.\n",
        "\n",
        "        Args:\n",
        "            image: Image à prédire\n",
        "\n",
        "        Returns:\n",
        "            tuple: (prédiction, probabilités, image_traitée, graphique)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            return \"❌ Modèle non chargé\", None, None, None\n",
        "\n",
        "        try:\n",
        "            # Prétraitement\n",
        "            processed_image = self.preprocess_image(image)\n",
        "\n",
        "            # Prédiction\n",
        "            probabilities = self.model.predict(processed_image, verbose=0)[0]\n",
        "            predicted_class = np.argmax(probabilities)\n",
        "            confidence = probabilities[predicted_class]\n",
        "\n",
        "            # Sauvegarder dans l'historique\n",
        "            self.prediction_history.append({\n",
        "                'timestamp': datetime.now().strftime(\"%H:%M:%S\"),\n",
        "                'prediction': int(predicted_class),\n",
        "                'confidence': float(confidence),\n",
        "                'probabilities': probabilities.tolist()\n",
        "            })\n",
        "\n",
        "            # Créer le graphique des probabilités\n",
        "            prob_chart = self.create_probability_chart(probabilities)\n",
        "\n",
        "            # Préparer l'image traitée pour affichage\n",
        "            display_image = (processed_image[0] * 255).astype(np.uint8)\n",
        "\n",
        "            # Résultat formaté\n",
        "            result = f\"🎯 **Prédiction: {predicted_class}**\\n\"\n",
        "            result += f\"🎲 **Confiance: {confidence:.2%}**\\n\\n\"\n",
        "            result += \"📊 **Top 3 Probabilités:**\\n\"\n",
        "\n",
        "            # Top 3 prédictions\n",
        "            top_3_idx = np.argsort(probabilities)[-3:][::-1]\n",
        "            for i, idx in enumerate(top_3_idx):\n",
        "                result += f\"{i+1}. Chiffre {idx}: {probabilities[idx]:.2%}\\n\"\n",
        "\n",
        "            return result, probabilities, display_image, prob_chart\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Erreur lors de la prédiction: {str(e)}\", None, None, None\n",
        "\n",
        "    def create_probability_chart(self, probabilities):\n",
        "        \"\"\"\n",
        "        Crée un graphique des probabilités.\n",
        "\n",
        "        Args:\n",
        "            probabilities: Array des probabilités\n",
        "\n",
        "        Returns:\n",
        "            matplotlib.figure: Graphique\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        classes = list(range(10))\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, 10))\n",
        "\n",
        "        bars = ax.bar(classes, probabilities, color=colors, alpha=0.8)\n",
        "\n",
        "        # Mettre en évidence la prédiction\n",
        "        max_idx = np.argmax(probabilities)\n",
        "        bars[max_idx].set_color('#ff6b6b')\n",
        "        bars[max_idx].set_alpha(1.0)\n",
        "\n",
        "        ax.set_xlabel('Chiffres', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Probabilité', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Distribution des Probabilités de Prédiction', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Ajouter les valeurs sur les barres\n",
        "        for i, (bar, prob) in enumerate(zip(bars, probabilities)):\n",
        "            if prob > 0.01:  # Afficher seulement si > 1%\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                       f'{prob:.1%}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def get_model_info(self):\n",
        "        \"\"\"\n",
        "        Retourne les informations du modèle.\n",
        "\n",
        "        Returns:\n",
        "            str: Informations formatées du modèle\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            return \"❌ Aucun modèle chargé\"\n",
        "\n",
        "        info = \"🤖 **Informations du Modèle**\\n\\n\"\n",
        "        info += f\"📈 **Précision de validation: {self.model_info['best_accuracy']:.4f}**\\n\"\n",
        "        info += f\"🔧 **Meilleurs paramètres:**\\n\"\n",
        "\n",
        "        params = self.model_info['best_params']\n",
        "        info += f\"   • Taux d'apprentissage: {params['learning_rate']}\\n\"\n",
        "        info += f\"   • Taille de batch: {params['batch_size']}\\n\"\n",
        "        info += f\"   • Couches cachées: {params['hidden_layers']}\\n\\n\"\n",
        "\n",
        "        info += f\"📊 **Architecture:**\\n\"\n",
        "        info += f\"   • Forme d'entrée: {self.model_info['input_shape']}\\n\"\n",
        "        info += f\"   • Nombre de classes: {self.model_info['num_classes']}\\n\"\n",
        "        info += f\"   • Nombre de paramètres: {self.model.count_params():,}\\n\"\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_prediction_history(self):\n",
        "        \"\"\"\n",
        "        Retourne l'historique des prédictions sous forme de DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Historique des prédictions\n",
        "        \"\"\"\n",
        "        if not self.prediction_history:\n",
        "            return pd.DataFrame(columns=['Heure', 'Prédiction', 'Confiance'])\n",
        "\n",
        "        df = pd.DataFrame(self.prediction_history)\n",
        "        df_display = pd.DataFrame({\n",
        "            'Heure': df['timestamp'],\n",
        "            'Prédiction': df['prediction'],\n",
        "            'Confiance': df['confidence'].apply(lambda x: f\"{x:.2%}\")\n",
        "        })\n",
        "\n",
        "        return df_display.tail(10)  # Dernières 10 prédictions\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"\n",
        "        Efface l'historique des prédictions.\n",
        "        \"\"\"\n",
        "        self.prediction_history = []\n",
        "        return pd.DataFrame(columns=['Heure', 'Prédiction', 'Confiance'])\n"
      ],
      "metadata": {
        "id": "L0ciblX-RBCO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"\n",
        "    Crée l'interface Gradio professionnelle.\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Interface Gradio configurée\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialiser le prédicteur\n",
        "    predictor = MNISTPredictor()\n",
        "\n",
        "    # Interface CSS personnalisé\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "    }\n",
        "\n",
        "    .main-header {\n",
        "        text-align: center;\n",
        "        color: white;\n",
        "        padding: 20px;\n",
        "        margin-bottom: 20px;\n",
        "        background: rgba(0,0,0,0.1);\n",
        "        border-radius: 10px;\n",
        "    }\n",
        "\n",
        "    .prediction-box {\n",
        "        background: rgba(255,255,255,0.9);\n",
        "        border-radius: 10px;\n",
        "        padding: 15px;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=custom_css, title=\"MNIST Digit Classifier\") as interface:\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"main-header\">\n",
        "            <h1>🔢 Classificateur de Chiffres Manuscrits MNIST</h1>\n",
        "            <p>Interface professionnelle utilisant un réseau de neurones TensorFlow</p>\n",
        "            <p><strong>Auteur:</strong> AKAKPO Koffi Moïse | <strong>Projet:</strong> Machine Learning Internship</p>\n",
        "            <p><em>💡 Astuce: Vous pouvez télécharger des images de chiffres manuscrits ou utiliser des captures d'écran</em></p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"🎯 Prédiction\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Zone de téléchargement d'image\n",
        "                    input_image = gr.Image(\n",
        "                        label=\"✏️ Téléchargez ou collez une image de chiffre (0-9)\",\n",
        "                        type=\"pil\",\n",
        "                        height=280,\n",
        "                        width=280\n",
        "                    )\n",
        "\n",
        "                    # Boutons\n",
        "                    with gr.Row():\n",
        "                        predict_btn = gr.Button(\"🔍 Prédire\", variant=\"primary\", size=\"lg\")\n",
        "                        clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    # Résultats\n",
        "                    prediction_output = gr.Markdown(\n",
        "                        label=\"📊 Résultat de la Prédiction\",\n",
        "                        value=\"👆 Téléchargez une image de chiffre et cliquez sur 'Prédire'\"\n",
        "                    )\n",
        "\n",
        "                    # Image traitée\n",
        "                    processed_image = gr.Image(\n",
        "                        label=\"🖼️ Image Traitée (28x28)\",\n",
        "                        type=\"numpy\"\n",
        "                    )\n",
        "\n",
        "            # Graphique des probabilités\n",
        "            probability_chart = gr.Plot(label=\"📈 Distribution des Probabilités\")\n",
        "\n",
        "        with gr.Tab(\"📊 Historique\"):\n",
        "            with gr.Row():\n",
        "                history_df = gr.Dataframe(\n",
        "                    label=\"📜 Dernières Prédictions\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                refresh_history_btn = gr.Button(\"🔄 Actualiser\", variant=\"secondary\")\n",
        "                clear_history_btn = gr.Button(\"🗑️ Effacer Historique\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Tab(\"ℹ️ Informations du Modèle\"):\n",
        "            model_info_output = gr.Markdown(\n",
        "                value=predictor.get_model_info(),\n",
        "                label=\"🤖 Détails du Modèle\"\n",
        "            )\n",
        "\n",
        "        # Gestion des événements\n",
        "        def predict_and_update(image):\n",
        "            if image is None:\n",
        "                return \"⚠️ Veuillez télécharger une image d'abord!\", None, None, None\n",
        "\n",
        "            result, probs, processed_img, chart = predictor.predict(image)\n",
        "            return result, processed_img, chart, predictor.get_prediction_history()\n",
        "\n",
        "        def clear_canvas():\n",
        "            return None, \"👆 Téléchargez une image de chiffre et cliquez sur 'Prédire'\", None, None\n",
        "\n",
        "        def refresh_history():\n",
        "            return predictor.get_prediction_history()\n",
        "\n",
        "        def clear_pred_history():\n",
        "            return predictor.clear_history()\n",
        "\n",
        "        # Connexions des événements\n",
        "        predict_btn.click(\n",
        "            fn=predict_and_update,\n",
        "            inputs=[input_image],\n",
        "            outputs=[prediction_output, processed_image, probability_chart, history_df]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_canvas,\n",
        "            outputs=[input_image, prediction_output, processed_image, probability_chart]\n",
        "        )\n",
        "\n",
        "        refresh_history_btn.click(\n",
        "            fn=refresh_history,\n",
        "            outputs=[history_df]\n",
        "        )\n",
        "\n",
        "        clear_history_btn.click(\n",
        "            fn=clear_pred_history,\n",
        "            outputs=[history_df]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Lance l'application Gradio.\n",
        "    \"\"\"\n",
        "    print(\"🚀 Lancement de l'interface MNIST Classifier...\")\n",
        "\n",
        "    # Créer et lancer l'interface\n",
        "    interface = create_gradio_interface()\n",
        "\n",
        "    # Lancer l'application\n",
        "    interface.launch(\n",
        "        share=True,          # Créer un lien public\n",
        "        server_name=\"0.0.0.0\",  # Accessible depuis toutes les IPs\n",
        "        server_port=7860,    # Port par défaut\n",
        "        show_error=True,     # Afficher les erreurs\n",
        "        quiet=False          # Mode verbose\n",
        "    )\n"
      ],
      "metadata": {
        "id": "zpD3snmYRGzm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "1ViJZtlgRKg1",
        "outputId": "ce12b034-362f-4c62-bf0e-01dcbfe039f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Lancement de l'interface MNIST Classifier...\n",
            "✅ Modèle chargé avec succès!\n",
            "   - Précision: 0.9805\n",
            "   - Paramètres: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3dc38f5646da82b337.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3dc38f5646da82b337.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}