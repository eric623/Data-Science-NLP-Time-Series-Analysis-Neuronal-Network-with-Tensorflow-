{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Neural Network Classification Project\n",
        "\n",
        "================================================================================\n",
        "\n",
        "A comprehensive implementation of feed-forward neural networks for image classification\n",
        "using TensorFlow/Keras on the MNIST dataset.\n",
        "\n",
        "Author: AKAKPO Koffi Mo√Øse\n",
        "\n",
        "Date: August 2025\n",
        "\n",
        "Purpose: Machine Learning Internship Application - Task 3"
      ],
      "metadata": {
        "id": "qTR8HrItm1YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installation de tensorflow\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r7ZguBBLn7jA",
        "outputId": "3afb56b5-6472-492a-ba1e-5051bad0f7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "s1JWXLyGnMh1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "paX1DFDYyikH"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkClassifier:\n",
        "    \"\"\"\n",
        "    A comprehensive neural network classifier with hyperparameter tuning capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize the neural network classifier.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of input data\n",
        "            num_classes (int): Number of output classes\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.best_model = None  # ‚úÖ Attribut pour stocker le meilleur mod√®le\n",
        "        self.best_params = None  # ‚úÖ Attribut pour stocker les meilleurs param√®tres\n",
        "        self.best_accuracy = 0   # ‚úÖ Attribut pour stocker la meilleure accuracy\n",
        "\n",
        "    def create_model(self, hidden_layers=[128, 64], dropout_rate=0.2, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Create a feed-forward neural network architecture.\n",
        "\n",
        "        Args:\n",
        "            hidden_layers (list): List of hidden layer sizes\n",
        "            dropout_rate (float): Dropout rate for regularization\n",
        "            learning_rate (float): Learning rate for optimizer\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Flatten(input_shape=self.input_shape),\n",
        "            layers.Dense(hidden_layers[0], activation='relu', name='hidden_1'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(hidden_layers[1], activation='relu', name='hidden_2'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(self.num_classes, activation='softmax', name='output')\n",
        "        ])\n",
        "\n",
        "        # Compile with appropriate optimizer and metrics\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, verbose=1):\n",
        "        \"\"\"\n",
        "        Train the neural network with early stopping and learning rate scheduling.\n",
        "\n",
        "        Args:\n",
        "            X_train, y_train: Training data\n",
        "            X_val, y_val: Validation data\n",
        "            epochs (int): Maximum number of epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            verbose (int): Verbosity level\n",
        "        \"\"\"\n",
        "        # Callbacks for better training\n",
        "        callbacks = [\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train the model\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data.\n",
        "\n",
        "        Args:\n",
        "            X_test, y_test: Test data\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred_proba = self.model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Classification report\n",
        "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # Confusion matrix\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        return {\n",
        "            'test_loss': test_loss,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_pred_proba,\n",
        "            'classification_report': class_report,\n",
        "            'confusion_matrix': conf_matrix\n",
        "        }\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train, y_train, X_val, y_val, param_grid=None):\n",
        "        \"\"\"\n",
        "        Perform hyperparameter tuning to find optimal parameters and save the best model.\n",
        "\n",
        "        Args:\n",
        "            X_train, y_train: Training data\n",
        "            X_val, y_val: Validation data\n",
        "            param_grid (dict): Dictionary of hyperparameters to test\n",
        "\n",
        "        Returns:\n",
        "            tuple: (best_params, results_df)\n",
        "        \"\"\"\n",
        "        print(\"Starting hyperparameter tuning...\")\n",
        "\n",
        "        # Default parameter grid if none provided\n",
        "        if param_grid is None:\n",
        "            param_grid = {\n",
        "                'learning_rate': [0.001, 0.01, 0.0001],\n",
        "                'batch_size': [32, 64, 128],\n",
        "                'hidden_layers': [[64, 32], [128, 64], [256, 128]]\n",
        "            }\n",
        "\n",
        "        best_accuracy = 0\n",
        "        best_params = {}\n",
        "        best_model = None\n",
        "        results = []\n",
        "\n",
        "        # Grid search\n",
        "        for lr in param_grid['learning_rate']:\n",
        "            for batch_size in param_grid['batch_size']:\n",
        "                for hidden_layers in param_grid['hidden_layers']:\n",
        "                    print(f\"Testing: LR={lr}, Batch={batch_size}, Hidden={hidden_layers}\")\n",
        "\n",
        "                    # Create and train model\n",
        "                    temp_model = self._create_temp_model(hidden_layers=hidden_layers, learning_rate=lr)\n",
        "\n",
        "                    history = temp_model.fit(\n",
        "                        X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=20,\n",
        "                        batch_size=batch_size,\n",
        "                        callbacks=[\n",
        "                            keras.callbacks.EarlyStopping(\n",
        "                                monitor='val_loss',\n",
        "                                patience=5,\n",
        "                                restore_best_weights=True,\n",
        "                                verbose=0\n",
        "                            )\n",
        "                        ],\n",
        "                        verbose=0\n",
        "                    )\n",
        "\n",
        "                    # Get best validation accuracy\n",
        "                    val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "                    results.append({\n",
        "                        'learning_rate': lr,\n",
        "                        'batch_size': batch_size,\n",
        "                        'hidden_layers': str(hidden_layers),\n",
        "                        'val_accuracy': val_accuracy\n",
        "                    })\n",
        "\n",
        "                    # ‚úÖ Update best model if current is better\n",
        "                    if val_accuracy > best_accuracy:\n",
        "                        best_accuracy = val_accuracy\n",
        "                        best_params = {\n",
        "                            'learning_rate': lr,\n",
        "                            'batch_size': batch_size,\n",
        "                            'hidden_layers': hidden_layers\n",
        "                        }\n",
        "                        # Clone the best model\n",
        "                        best_model = tf.keras.models.clone_model(temp_model)\n",
        "                        best_model.set_weights(temp_model.get_weights())\n",
        "                        best_model.compile(\n",
        "                            optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                            loss='sparse_categorical_crossentropy',\n",
        "                            metrics=['accuracy']\n",
        "                        )\n",
        "\n",
        "        # ‚úÖ Store best results in instance attributes\n",
        "        self.best_model = best_model\n",
        "        self.best_params = best_params\n",
        "        self.best_accuracy = best_accuracy\n",
        "\n",
        "        # Convert results to DataFrame for better visualization\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nHyperparameter Tuning Results:\")\n",
        "        print(results_df.sort_values('val_accuracy', ascending=False))\n",
        "\n",
        "        print(f\"\\nBest Parameters: {best_params}\")\n",
        "        print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "        return best_params, results_df\n",
        "\n",
        "    def _create_temp_model(self, hidden_layers=[128, 64], dropout_rate=0.2, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Create a temporary model for hyperparameter tuning.\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Flatten(input_shape=self.input_shape),\n",
        "            layers.Dense(hidden_layers[0], activation='relu', name='hidden_1'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(hidden_layers[1], activation='relu', name='hidden_2'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(self.num_classes, activation='softmax', name='output')\n",
        "        ])\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_best_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the best model to a file using pickle.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the model\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No best model found. Run hyperparameter_tuning first.\")\n",
        "\n",
        "        model_data = {\n",
        "            'model': self.best_model,\n",
        "            'best_params': self.best_params,\n",
        "            'best_accuracy': self.best_accuracy,\n",
        "            'input_shape': self.input_shape,\n",
        "            'num_classes': self.num_classes\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(model_data, f)\n",
        "\n",
        "        print(f\"‚úÖ Best model saved to: {filepath}\")\n",
        "        print(f\"   - Accuracy: {self.best_accuracy:.4f}\")\n",
        "        print(f\"   - Parameters: {self.best_params}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_best_model(cls, filepath):\n",
        "        \"\"\"\n",
        "        Load a saved best model.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the saved model\n",
        "\n",
        "        Returns:\n",
        "            NeuralNetworkClassifier: Loaded classifier instance\n",
        "        \"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            model_data = pickle.load(f)\n",
        "\n",
        "        # Create new instance\n",
        "        classifier = cls(model_data['input_shape'], model_data['num_classes'])\n",
        "        classifier.best_model = model_data['model']\n",
        "        classifier.best_params = model_data['best_params']\n",
        "        classifier.best_accuracy = model_data['best_accuracy']\n",
        "\n",
        "        print(f\"‚úÖ Best model loaded from: {filepath}\")\n",
        "        print(f\"   - Accuracy: {classifier.best_accuracy:.4f}\")\n",
        "        print(f\"   - Parameters: {classifier.best_params}\")\n",
        "\n",
        "        return classifier\n",
        "\n",
        "    def predict_with_best_model(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the best model.\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "\n",
        "        Returns:\n",
        "            dict: Predictions and probabilities\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No best model found. Run hyperparameter_tuning first.\")\n",
        "\n",
        "        probabilities = self.best_model.predict(X, verbose=0)\n",
        "        predictions = np.argmax(probabilities, axis=1)\n",
        "\n",
        "        return {\n",
        "            'predictions': predictions,\n",
        "            'probabilities': probabilities\n",
        "        }\n",
        "\n",
        "    def get_model_summary(self):\n",
        "        \"\"\"\n",
        "        Get a summary of the best model and parameters.\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary information\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            return {\"status\": \"No best model found. Run hyperparameter_tuning first.\"}\n",
        "\n",
        "        return {\n",
        "            'status': 'Best model available',\n",
        "            'best_accuracy': self.best_accuracy,\n",
        "            'best_params': self.best_params,\n",
        "            'input_shape': self.input_shape,\n",
        "            'num_classes': self.num_classes,\n",
        "            'model_layers': [layer.name for layer in self.best_model.layers]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Load and preprocess the MNIST dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Preprocessed training and testing data\n",
        "    \"\"\"\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "\n",
        "    # Load data\n",
        "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Split training data into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"Image shape: {X_train.shape[1:]}\")\n",
        "    print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Exemple d'utilisation avec sauvegarde du meilleur mod√®le.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"NEURAL NETWORK CLASSIFICATION PROJECT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_and_preprocess_data()\n",
        "\n",
        "    # Create classifier\n",
        "    nn_classifier = NeuralNetworkClassifier(input_shape=(28, 28), num_classes=10)\n",
        "\n",
        "    # Hyperparameter tuning (automatically saves best model)\n",
        "    print(\"\\n2. Hyperparameter Tuning\")\n",
        "    best_params, tuning_results = nn_classifier.hyperparameter_tuning(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # ‚úÖ Save the best model\n",
        "    nn_classifier.save_best_model('best_mnist_model.pkl')\n",
        "\n",
        "    # ‚úÖ Test loading the model\n",
        "    print(\"\\n3. Testing Model Loading\")\n",
        "    loaded_classifier = NeuralNetworkClassifier.load_best_model('best_mnist_model.pkl')\n",
        "\n",
        "    # ‚úÖ Make predictions with best model\n",
        "    print(\"\\n4. Making Predictions with Best Model\")\n",
        "    results = loaded_classifier.predict_with_best_model(X_test[:100])\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = loaded_classifier.best_model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy with Best Model: {test_accuracy:.4f}\")\n",
        "\n",
        "    # ‚úÖ Display model summary\n",
        "    print(\"\\n5. Model Summary\")\n",
        "    summary = loaded_classifier.get_model_summary()\n",
        "    for key, value in summary.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ BEST MODEL SUCCESSFULLY SAVED AND LOADED!\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "5fHHux8t7iZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCQdBvCKmu3M",
        "outputId": "8f96d2ad-3f95-4e9f-dfd3-eaeff629b649"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "NEURAL NETWORK CLASSIFICATION PROJECT\n",
            "============================================================\n",
            "Loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training set: 48000 samples\n",
            "Validation set: 12000 samples\n",
            "Test set: 10000 samples\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 10\n",
            "\n",
            "2. Hyperparameter Tuning\n",
            "Starting hyperparameter tuning...\n",
            "Testing: LR=0.001, Batch=32, Hidden=[64, 32]\n",
            "Testing: LR=0.001, Batch=32, Hidden=[128, 64]\n",
            "Testing: LR=0.001, Batch=32, Hidden=[256, 128]\n",
            "Testing: LR=0.001, Batch=64, Hidden=[64, 32]\n",
            "Testing: LR=0.001, Batch=64, Hidden=[128, 64]\n",
            "Testing: LR=0.001, Batch=64, Hidden=[256, 128]\n",
            "Testing: LR=0.001, Batch=128, Hidden=[64, 32]\n",
            "Testing: LR=0.001, Batch=128, Hidden=[128, 64]\n",
            "Testing: LR=0.001, Batch=128, Hidden=[256, 128]\n",
            "Testing: LR=0.01, Batch=32, Hidden=[64, 32]\n",
            "Testing: LR=0.01, Batch=32, Hidden=[128, 64]\n",
            "Testing: LR=0.01, Batch=32, Hidden=[256, 128]\n",
            "Testing: LR=0.01, Batch=64, Hidden=[64, 32]\n",
            "Testing: LR=0.01, Batch=64, Hidden=[128, 64]\n",
            "Testing: LR=0.01, Batch=64, Hidden=[256, 128]\n",
            "Testing: LR=0.01, Batch=128, Hidden=[64, 32]\n",
            "Testing: LR=0.01, Batch=128, Hidden=[128, 64]\n",
            "Testing: LR=0.01, Batch=128, Hidden=[256, 128]\n",
            "Testing: LR=0.0001, Batch=32, Hidden=[64, 32]\n",
            "Testing: LR=0.0001, Batch=32, Hidden=[128, 64]\n",
            "Testing: LR=0.0001, Batch=32, Hidden=[256, 128]\n",
            "Testing: LR=0.0001, Batch=64, Hidden=[64, 32]\n",
            "Testing: LR=0.0001, Batch=64, Hidden=[128, 64]\n",
            "Testing: LR=0.0001, Batch=64, Hidden=[256, 128]\n",
            "Testing: LR=0.0001, Batch=128, Hidden=[64, 32]\n",
            "Testing: LR=0.0001, Batch=128, Hidden=[128, 64]\n",
            "Testing: LR=0.0001, Batch=128, Hidden=[256, 128]\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "    learning_rate  batch_size hidden_layers  val_accuracy\n",
            "8          0.0010         128    [256, 128]      0.980500\n",
            "5          0.0010          64    [256, 128]      0.979833\n",
            "1          0.0010          32     [128, 64]      0.979333\n",
            "4          0.0010          64     [128, 64]      0.979250\n",
            "20         0.0001          32    [256, 128]      0.979083\n",
            "7          0.0010         128     [128, 64]      0.978333\n",
            "2          0.0010          32    [256, 128]      0.978167\n",
            "23         0.0001          64    [256, 128]      0.977750\n",
            "19         0.0001          32     [128, 64]      0.974250\n",
            "6          0.0010         128      [64, 32]      0.974083\n",
            "3          0.0010          64      [64, 32]      0.973333\n",
            "26         0.0001         128    [256, 128]      0.973167\n",
            "0          0.0010          32      [64, 32]      0.971583\n",
            "17         0.0100         128    [256, 128]      0.970000\n",
            "22         0.0001          64     [128, 64]      0.969583\n",
            "16         0.0100         128     [128, 64]      0.967333\n",
            "13         0.0100          64     [128, 64]      0.965083\n",
            "15         0.0100         128      [64, 32]      0.964750\n",
            "25         0.0001         128     [128, 64]      0.963417\n",
            "14         0.0100          64    [256, 128]      0.963417\n",
            "18         0.0001          32      [64, 32]      0.962583\n",
            "10         0.0100          32     [128, 64]      0.958417\n",
            "12         0.0100          64      [64, 32]      0.958083\n",
            "21         0.0001          64      [64, 32]      0.955583\n",
            "11         0.0100          32    [256, 128]      0.952750\n",
            "9          0.0100          32      [64, 32]      0.949500\n",
            "24         0.0001         128      [64, 32]      0.948083\n",
            "\n",
            "Best Parameters: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "Best Validation Accuracy: 0.9805\n",
            "‚úÖ Best model saved to: best_mnist_model.pkl\n",
            "   - Accuracy: 0.9805\n",
            "   - Parameters: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "\n",
            "3. Testing Model Loading\n",
            "‚úÖ Best model loaded from: best_mnist_model.pkl\n",
            "   - Accuracy: 0.9805\n",
            "   - Parameters: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "\n",
            "4. Making Predictions with Best Model\n",
            "Test Accuracy with Best Model: 0.9787\n",
            "\n",
            "5. Model Summary\n",
            "status: Best model available\n",
            "best_accuracy: 0.9804999828338623\n",
            "best_params: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "input_shape: (28, 28)\n",
            "num_classes: 10\n",
            "model_layers: ['flatten_8', 'hidden_1', 'dropout_16', 'hidden_2', 'dropout_17', 'output']\n",
            "\n",
            "============================================================\n",
            "‚úÖ BEST MODEL SUCCESSFULLY SAVED AND LOADED!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTPredictor:\n",
        "    \"\"\"\n",
        "    Interface professionnelle pour la pr√©diction de chiffres manuscrits MNIST.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path='best_mnist_model.pkl'):\n",
        "        \"\"\"\n",
        "        Initialise le pr√©dicteur avec le mod√®le sauvegard√©.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Chemin vers le mod√®le sauvegard√©\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        self.model_info = None\n",
        "        self.prediction_history = []\n",
        "\n",
        "        # Charger le mod√®le\n",
        "        self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Charge le mod√®le TensorFlow depuis le fichier pickle.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Chemin vers le mod√®le sauvegard√©\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(model_path, 'rb') as f:\n",
        "                model_data = pickle.load(f)\n",
        "\n",
        "            self.model = model_data['model']\n",
        "            self.model_info = {\n",
        "                'best_accuracy': model_data['best_accuracy'],\n",
        "                'best_params': model_data['best_params'],\n",
        "                'input_shape': model_data['input_shape'],\n",
        "                'num_classes': model_data['num_classes']\n",
        "            }\n",
        "\n",
        "            print(f\"‚úÖ Mod√®le charg√© avec succ√®s!\")\n",
        "            print(f\"   - Pr√©cision: {self.model_info['best_accuracy']:.4f}\")\n",
        "            print(f\"   - Param√®tres: {self.model_info['best_params']}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Fichier mod√®le non trouv√©: {model_path}\")\n",
        "            print(\"   Assurez-vous d'avoir ex√©cut√© l'entra√Ænement d'abord.\")\n",
        "            self.model = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors du chargement du mod√®le: {e}\")\n",
        "            self.model = None\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"\n",
        "        Pr√©traite l'image pour la pr√©diction.\n",
        "\n",
        "        Args:\n",
        "            image: Image PIL ou array numpy\n",
        "\n",
        "        Returns:\n",
        "            np.array: Image pr√©trait√©e\n",
        "        \"\"\"\n",
        "        # Convertir PIL en numpy si n√©cessaire\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Convertir en niveaux de gris si couleur\n",
        "        if len(image.shape) == 3:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Redimensionner √† 28x28\n",
        "        image = cv2.resize(image, (28, 28))\n",
        "\n",
        "        # Inverser les couleurs (fond noir, √©criture blanche)\n",
        "        image = 255 - image\n",
        "\n",
        "        # Normaliser entre 0 et 1\n",
        "        image = image.astype('float32') / 255.0\n",
        "\n",
        "        # Ajouter dimension batch\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def predict(self, image):\n",
        "        \"\"\"\n",
        "        Effectue la pr√©diction sur l'image.\n",
        "\n",
        "        Args:\n",
        "            image: Image √† pr√©dire\n",
        "\n",
        "        Returns:\n",
        "            tuple: (pr√©diction, probabilit√©s, image_trait√©e, graphique)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            return \"‚ùå Mod√®le non charg√©\", None, None, None\n",
        "\n",
        "        try:\n",
        "            # Pr√©traitement\n",
        "            processed_image = self.preprocess_image(image)\n",
        "\n",
        "            # Pr√©diction\n",
        "            probabilities = self.model.predict(processed_image, verbose=0)[0]\n",
        "            predicted_class = np.argmax(probabilities)\n",
        "            confidence = probabilities[predicted_class]\n",
        "\n",
        "            # Sauvegarder dans l'historique\n",
        "            self.prediction_history.append({\n",
        "                'timestamp': datetime.now().strftime(\"%H:%M:%S\"),\n",
        "                'prediction': int(predicted_class),\n",
        "                'confidence': float(confidence),\n",
        "                'probabilities': probabilities.tolist()\n",
        "            })\n",
        "\n",
        "            # Cr√©er le graphique des probabilit√©s\n",
        "            prob_chart = self.create_probability_chart(probabilities)\n",
        "\n",
        "            # Pr√©parer l'image trait√©e pour affichage\n",
        "            display_image = (processed_image[0] * 255).astype(np.uint8)\n",
        "\n",
        "            # R√©sultat format√©\n",
        "            result = f\"üéØ **Pr√©diction: {predicted_class}**\\n\"\n",
        "            result += f\"üé≤ **Confiance: {confidence:.2%}**\\n\\n\"\n",
        "            result += \"üìä **Top 3 Probabilit√©s:**\\n\"\n",
        "\n",
        "            # Top 3 pr√©dictions\n",
        "            top_3_idx = np.argsort(probabilities)[-3:][::-1]\n",
        "            for i, idx in enumerate(top_3_idx):\n",
        "                result += f\"{i+1}. Chiffre {idx}: {probabilities[idx]:.2%}\\n\"\n",
        "\n",
        "            return result, probabilities, display_image, prob_chart\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Erreur lors de la pr√©diction: {str(e)}\", None, None, None\n",
        "\n",
        "    def create_probability_chart(self, probabilities):\n",
        "        \"\"\"\n",
        "        Cr√©e un graphique des probabilit√©s.\n",
        "\n",
        "        Args:\n",
        "            probabilities: Array des probabilit√©s\n",
        "\n",
        "        Returns:\n",
        "            matplotlib.figure: Graphique\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        classes = list(range(10))\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, 10))\n",
        "\n",
        "        bars = ax.bar(classes, probabilities, color=colors, alpha=0.8)\n",
        "\n",
        "        # Mettre en √©vidence la pr√©diction\n",
        "        max_idx = np.argmax(probabilities)\n",
        "        bars[max_idx].set_color('#ff6b6b')\n",
        "        bars[max_idx].set_alpha(1.0)\n",
        "\n",
        "        ax.set_xlabel('Chiffres', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Probabilit√©', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Distribution des Probabilit√©s de Pr√©diction', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Ajouter les valeurs sur les barres\n",
        "        for i, (bar, prob) in enumerate(zip(bars, probabilities)):\n",
        "            if prob > 0.01:  # Afficher seulement si > 1%\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                       f'{prob:.1%}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def get_model_info(self):\n",
        "        \"\"\"\n",
        "        Retourne les informations du mod√®le.\n",
        "\n",
        "        Returns:\n",
        "            str: Informations format√©es du mod√®le\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            return \"‚ùå Aucun mod√®le charg√©\"\n",
        "\n",
        "        info = \"ü§ñ **Informations du Mod√®le**\\n\\n\"\n",
        "        info += f\"üìà **Pr√©cision de validation: {self.model_info['best_accuracy']:.4f}**\\n\"\n",
        "        info += f\"üîß **Meilleurs param√®tres:**\\n\"\n",
        "\n",
        "        params = self.model_info['best_params']\n",
        "        info += f\"   ‚Ä¢ Taux d'apprentissage: {params['learning_rate']}\\n\"\n",
        "        info += f\"   ‚Ä¢ Taille de batch: {params['batch_size']}\\n\"\n",
        "        info += f\"   ‚Ä¢ Couches cach√©es: {params['hidden_layers']}\\n\\n\"\n",
        "\n",
        "        info += f\"üìä **Architecture:**\\n\"\n",
        "        info += f\"   ‚Ä¢ Forme d'entr√©e: {self.model_info['input_shape']}\\n\"\n",
        "        info += f\"   ‚Ä¢ Nombre de classes: {self.model_info['num_classes']}\\n\"\n",
        "        info += f\"   ‚Ä¢ Nombre de param√®tres: {self.model.count_params():,}\\n\"\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_prediction_history(self):\n",
        "        \"\"\"\n",
        "        Retourne l'historique des pr√©dictions sous forme de DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Historique des pr√©dictions\n",
        "        \"\"\"\n",
        "        if not self.prediction_history:\n",
        "            return pd.DataFrame(columns=['Heure', 'Pr√©diction', 'Confiance'])\n",
        "\n",
        "        df = pd.DataFrame(self.prediction_history)\n",
        "        df_display = pd.DataFrame({\n",
        "            'Heure': df['timestamp'],\n",
        "            'Pr√©diction': df['prediction'],\n",
        "            'Confiance': df['confidence'].apply(lambda x: f\"{x:.2%}\")\n",
        "        })\n",
        "\n",
        "        return df_display.tail(10)  # Derni√®res 10 pr√©dictions\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"\n",
        "        Efface l'historique des pr√©dictions.\n",
        "        \"\"\"\n",
        "        self.prediction_history = []\n",
        "        return pd.DataFrame(columns=['Heure', 'Pr√©diction', 'Confiance'])\n"
      ],
      "metadata": {
        "id": "L0ciblX-RBCO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"\n",
        "    Cr√©e l'interface Gradio professionnelle.\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Interface Gradio configur√©e\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialiser le pr√©dicteur\n",
        "    predictor = MNISTPredictor()\n",
        "\n",
        "    # Interface CSS personnalis√©\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "    }\n",
        "\n",
        "    .main-header {\n",
        "        text-align: center;\n",
        "        color: white;\n",
        "        padding: 20px;\n",
        "        margin-bottom: 20px;\n",
        "        background: rgba(0,0,0,0.1);\n",
        "        border-radius: 10px;\n",
        "    }\n",
        "\n",
        "    .prediction-box {\n",
        "        background: rgba(255,255,255,0.9);\n",
        "        border-radius: 10px;\n",
        "        padding: 15px;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=custom_css, title=\"MNIST Digit Classifier\") as interface:\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"main-header\">\n",
        "            <h1>üî¢ Classificateur de Chiffres Manuscrits MNIST</h1>\n",
        "            <p>Interface professionnelle utilisant un r√©seau de neurones TensorFlow</p>\n",
        "            <p><strong>Auteur:</strong> AKAKPO Koffi Mo√Øse | <strong>Projet:</strong> Machine Learning Internship</p>\n",
        "            <p><em>üí° Astuce: Vous pouvez t√©l√©charger des images de chiffres manuscrits ou utiliser des captures d'√©cran</em></p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"üéØ Pr√©diction\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Zone de t√©l√©chargement d'image\n",
        "                    input_image = gr.Image(\n",
        "                        label=\"‚úèÔ∏è T√©l√©chargez ou collez une image de chiffre (0-9)\",\n",
        "                        type=\"pil\",\n",
        "                        height=280,\n",
        "                        width=280\n",
        "                    )\n",
        "\n",
        "                    # Boutons\n",
        "                    with gr.Row():\n",
        "                        predict_btn = gr.Button(\"üîç Pr√©dire\", variant=\"primary\", size=\"lg\")\n",
        "                        clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    # R√©sultats\n",
        "                    prediction_output = gr.Markdown(\n",
        "                        label=\"üìä R√©sultat de la Pr√©diction\",\n",
        "                        value=\"üëÜ T√©l√©chargez une image de chiffre et cliquez sur 'Pr√©dire'\"\n",
        "                    )\n",
        "\n",
        "                    # Image trait√©e\n",
        "                    processed_image = gr.Image(\n",
        "                        label=\"üñºÔ∏è Image Trait√©e (28x28)\",\n",
        "                        type=\"numpy\"\n",
        "                    )\n",
        "\n",
        "            # Graphique des probabilit√©s\n",
        "            probability_chart = gr.Plot(label=\"üìà Distribution des Probabilit√©s\")\n",
        "\n",
        "        with gr.Tab(\"üìä Historique\"):\n",
        "            with gr.Row():\n",
        "                history_df = gr.Dataframe(\n",
        "                    label=\"üìú Derni√®res Pr√©dictions\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                refresh_history_btn = gr.Button(\"üîÑ Actualiser\", variant=\"secondary\")\n",
        "                clear_history_btn = gr.Button(\"üóëÔ∏è Effacer Historique\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Tab(\"‚ÑπÔ∏è Informations du Mod√®le\"):\n",
        "            model_info_output = gr.Markdown(\n",
        "                value=predictor.get_model_info(),\n",
        "                label=\"ü§ñ D√©tails du Mod√®le\"\n",
        "            )\n",
        "\n",
        "        # Gestion des √©v√©nements\n",
        "        def predict_and_update(image):\n",
        "            if image is None:\n",
        "                return \"‚ö†Ô∏è Veuillez t√©l√©charger une image d'abord!\", None, None, None\n",
        "\n",
        "            result, probs, processed_img, chart = predictor.predict(image)\n",
        "            return result, processed_img, chart, predictor.get_prediction_history()\n",
        "\n",
        "        def clear_canvas():\n",
        "            return None, \"üëÜ T√©l√©chargez une image de chiffre et cliquez sur 'Pr√©dire'\", None, None\n",
        "\n",
        "        def refresh_history():\n",
        "            return predictor.get_prediction_history()\n",
        "\n",
        "        def clear_pred_history():\n",
        "            return predictor.clear_history()\n",
        "\n",
        "        # Connexions des √©v√©nements\n",
        "        predict_btn.click(\n",
        "            fn=predict_and_update,\n",
        "            inputs=[input_image],\n",
        "            outputs=[prediction_output, processed_image, probability_chart, history_df]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_canvas,\n",
        "            outputs=[input_image, prediction_output, processed_image, probability_chart]\n",
        "        )\n",
        "\n",
        "        refresh_history_btn.click(\n",
        "            fn=refresh_history,\n",
        "            outputs=[history_df]\n",
        "        )\n",
        "\n",
        "        clear_history_btn.click(\n",
        "            fn=clear_pred_history,\n",
        "            outputs=[history_df]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Lance l'application Gradio.\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Lancement de l'interface MNIST Classifier...\")\n",
        "\n",
        "    # Cr√©er et lancer l'interface\n",
        "    interface = create_gradio_interface()\n",
        "\n",
        "    # Lancer l'application\n",
        "    interface.launch(\n",
        "        share=True,          # Cr√©er un lien public\n",
        "        server_name=\"0.0.0.0\",  # Accessible depuis toutes les IPs\n",
        "        server_port=7860,    # Port par d√©faut\n",
        "        show_error=True,     # Afficher les erreurs\n",
        "        quiet=False          # Mode verbose\n",
        "    )\n"
      ],
      "metadata": {
        "id": "zpD3snmYRGzm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "1ViJZtlgRKg1",
        "outputId": "ce12b034-362f-4c62-bf0e-01dcbfe039f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Lancement de l'interface MNIST Classifier...\n",
            "‚úÖ Mod√®le charg√© avec succ√®s!\n",
            "   - Pr√©cision: 0.9805\n",
            "   - Param√®tres: {'learning_rate': 0.001, 'batch_size': 128, 'hidden_layers': [256, 128]}\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3dc38f5646da82b337.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3dc38f5646da82b337.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}